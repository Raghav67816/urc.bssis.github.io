{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Bio-Silicon Synergetic Intelligence System"},{"location":"#project-overview","title":"\ud83e\udde0 Project Overview","text":"<p>At Synthetic Intelligence Labs, our mission is to harmonize biological cognition with computational rigor. We have embarked on an innovative venture that intricately fuses human cortical organoids with rat brains, unveiling a new era of bio-silicon synergetic learning. Our bespoke BCI system is a testament to this, with carbon nanotube-coated electrodes at its core, enhancing the fidelity of neural interfacing through self-optimizing signal pathways.</p> <p>Our methodology is deeply rooted in bidirectional communication, leveraging AI to map cerebral signals onto interactive platforms, exemplified by our translation of neural impulses into gameplay dynamics. This iterative learning cycle commences at the cortical interface, progressing through a FreeEEG32 board, interfacing with our proprietary BrainFlow acquisition system, and culminating in our custom software. Here, our GUI provides an immersive analytical experience, inclusive of innovative visualizations such as the phase synchronization vortex.</p>"},{"location":"#system-configuration","title":"System Configuration","text":"<ul> <li>Brain Surface Communication: Neural activities are mapped via AI, translating into game movements and vice versa.</li> <li>Signal Transmission: The process begins with brain surface signals, read from our MEA, transmitted through a FreeEEG32 board, to BrainFlow for acquisition, then into our custom software for analysis.</li> <li>Neuromimetic Feedback: Neural signals are decoded into game actions, with game and in-game rat movement data (facilitating a self-loop learning concept) encoded back into neuromimetic signals. These signals are then fed back into the rat brain.</li> <li>Signal Processing: The 32 signals are sent to two 16 port usb hubs, connected to modified usb-audio converters, followed by resistors for voltage division to match ECoG voltage levels.</li> </ul>"},{"location":"#microelectrode-array-mea-specifications","title":"Microelectrode Array (MEA) Specifications","text":""},{"location":"#electrode-specifications","title":"Electrode Specifications","text":"<ul> <li>Type: 64 electrodes (32 input/recording, 32 output/stimulation).</li> <li>Wire Gauge: 30 AWG (254.6 micrometers diameter).</li> <li>Array Area: 3 cm\u00b2 total coverage.</li> <li>Spatial Resolution: 400-500 micrometers.</li> <li>Temporal Resolution: 1000 - 2000 Hz.</li> </ul>"},{"location":"#encasement-and-design","title":"Encasement and Design","text":"<ul> <li>Material: Medical-grade silicone for flexibility and biocompatibility.</li> <li>Thickness: 0.1 mm, accommodating brain tissue growth.</li> <li>Shape: Two connected trapezoids for conforming to brain curvature.</li> <li>Wiring: Twisted pair configuration for each electrode, reducing interference.</li> </ul>"},{"location":"#papers","title":"Papers","text":"<ul> <li>Phase 1: Software Development and Prospective Implantation of Microelectrode Arrays and Human Cortical Organoids into Rat Brains</li> </ul>"},{"location":"#rat-and-computer-learning","title":"Rat and Computer Learning","text":"<p>The system we've developed intricately blends advanced software automation with a nuanced understanding of rat behavior and neuroscience. Central to this system is the dual approach of reward and deterrent signaling, tailored specifically for the rat's unique sensory and cognitive processing.</p> <ul> <li> <p>Reward Mechanism: At the heart of positive reinforcement, our automated fluidics system is programmed to deliver a carefully formulated reward solution to the rat. This concoction, a precise blend of sucrose, sodium chloride, nicotine, and caffeine, is designed to stimulate the rat's reward centers, promoting engagement and positive response patterns. This aspect of the system is critical for encouraging the desired behaviors in the rat through natural, positive stimuli.</p> </li> <li> <p>Deterrent Signaling: Complementing the reward system is the deterrent mechanism, which employs audio signals beyond human auditory perception to subtly influence the rat's behavior. These human-inaudible distress sounds are calibrated to create a mild sense of unease or alertness in the rat, without causing undue stress or harm. This auditory deterrent is a key component in shaping the rat's behavior, helping to guide it away from undesirable actions or responses.</p> </li> <li> <p>AI-Driven Supervised Learning Framework: The convergence of these two systems is overseen and optimized by our sophisticated AI software. This AI component initiates the process through a phase of supervised learning, wherein incoming neural signals and corresponding actions are meticulously analyzed. The system then generates metadata-rich outgoing signals, which are fine-tuned to enhance the learning and adaptation process. This initial phase of supervised learning is crucial for establishing a robust foundation for the system's AI to learn, adapt, and evolve in response to the rat's neural patterns and behaviors.</p> </li> </ul> <p>In essence, our system represents a harmonious fusion of biotechnology and artificial intelligence, designed to explore and expand the boundaries of neuroscientific research and animal behavior understanding. This dynamic, responsive system is poised to offer unprecedented insights into neural processing, learning mechanisms, and the complex interplay between biological entities and computational intelligence.</p>"},{"location":"#doom-system","title":"DOOM system","text":"<p>In our system, we will leverage the combined capabilities of VizDoom and Gymnasium to access and utilize comprehensive game state information for effective decision-making and analysis. VizDoom will serve as our primary interface for interacting with the Doom game engine, providing us with rich access to various game state data such as player status, enemy positions, level layouts, weapon information, and observation spaces. Through Gymnasium, we will create custom gym environments tailored to our specific scenarios and objectives, allowing us to seamlessly integrate VizDoom's functionalities into our reinforcement learning pipelines. By harnessing the power of VizDoom and Gymnasium together, we aim to develop robust AI agents capable of understanding and navigating complex game environments, adapting their strategies based on real-time game state observations, and ultimately achieving specified objectives within the Doom universe.</p>"},{"location":"#subject-health-and-wellbeing","title":"Subject Health and Wellbeing","text":"<p>Central to our research ethos is the holistic well-being of the rats involved in our study. Recognizing the importance of social structures in the health and well-being of these animals, all rats are housed together in a communal environment. This approach not only supports their social health but also fosters a more natural living condition, crucial for their overall welfare.</p> <p>Nutrition is another cornerstone of our care regimen. The diet for these rats is meticulously planned and includes a rich variety of superfruits and fish oil supplements. This diet is designed to ensure optimal health, providing essential nutrients and antioxidants that support their cognitive and physical well-being.</p> <p>Furthermore, our commitment extends beyond the confines of the laboratory. We ensure that all rats exiting our wetlab are in robust health. Their release is carefully orchestrated, with a focus on their long-term welfare. They are released responsibly, in groups, to support their social nature and ease their transition back into a natural habitat. This practice underscores our dedication to ethical research and the humane treatment of all animals involved in our studies.</p> <p>We invite researchers to join us in this groundbreaking journey, to collaborate and contribute to the evolution of synthetic biological intelligence. Engage with us, and let\u2019s shape the future of brain-computer interfaces together.</p>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features:","text":"<ul> <li>Tailored BCI &amp; MEA: Designed specifically for rat brains, enhancing neural connectivity and signal precision.</li> <li>Carbon Nanotube Technology: Exploits the adaptive capabilities of rat brains and CNTs for superior signal quality.</li> <li>Neural-Computational Language: Developing a novel symbolic language to streamline brain-computer communication, especially in gaming contexts.</li> </ul>"},{"location":"#collaborative-milestones","title":"\ud83e\udd1d Collaborative Milestones:","text":"<ul> <li>University of Michigan: Advancing optical stimulation in \"DishBrain\" experiment replicas.</li> <li>FinalSpark: Delving into human cortical spheroid learning mechanisms.</li> <li>University of Reading: Innovative use of bacteria in neural networks.</li> <li>City, University of London: Systems and states, including harmonics in TES EEG data.</li> </ul>"},{"location":"#inviting-collaboration","title":"\ud83d\udca1 Inviting Collaboration","text":"<p>We're reaching out to like-minded researchers and innovators to join us on this journey. Your expertise in BCI could be the catalyst for unprecedented breakthroughs. Let's explore the synergy between our visions and set new benchmarks in BCI technology.</p>"},{"location":"#join-us","title":"\ud83d\udce2 Join Us:","text":"<ul> <li>Discord: Connect with us on Discord and become part of a vibrant community shaping the future.</li> </ul>"},{"location":"#connect","title":"\ud83e\udd1d Connect","text":"<p>We're more than a project; we're a movement. Let's make history together. Get in touch!</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.</p>"},{"location":"#related-projects","title":"\ud83e\uddec Related Projects","text":"<ul> <li>Human Cortical Organoid Signal Analysis: Signal analysis and prediction of brain signals, adaptable for various signal types. PyPI libraries available from our experiments.</li> <li>EEG Prediction with Chaos Theory: Leveraging chaos theory and a CNN Kuramoto transformer RNN for signal prediction. Includes PyPI library implementations.</li> <li>Bacteria Neural Network (Upcoming): An innovative approach using bacteria as functional components of a neural network. Collaboration between Synthetic Intelligence Labs and Complex Living Machines Lab Dr. Yoshikatsu Hayashi</li> </ul>"},{"location":"#contact","title":"\ud83d\udce9 Contact","text":"<p>For collaborations, press inquiries, or questions: - Email: soul.syrup@yandex.com or soul.syrupp@gmail.com - Discord: soul_syrup</p>"},{"location":"#library-testing-invitation","title":"\ud83d\udcda Library Testing Invitation","text":"<p>We invite you to test our PyPI library for human brain cortical organoid/spheroid, EEG, ECoG, and other signal analyses: - Neural Signal Analysis Library from Synthetic Intelligence Labs.</p> <p>Human-Brain-Rat by Synthetic Intelligence Labs is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.Based on a work at Synthetic Intelligence Labs.Permissions beyond the scope of this license may be available at Synthetic Intelligence Labs.</p>"},{"location":"System%20Design/","title":"System Design","text":"<p>This research involves complex software worflow. Therefore, it becomes essential to visualize the process. </p> <p></p> <ul> <li>Step 1: Raw EEG signals are fetched over 32 channels from the subject.</li> <li>Step 2: The recieved analog signals are then converted into digital signals.</li> <li>Step 3: In this step the digital signals are decoding into movements using features (e.g move forward or backward, turn left or right).</li> <li>Step 4: VizDoom has its own set of commands to perform defined action inside of the game environment. The movements are converted into VizDoom commands.</li> </ul> <p>At each step some mathematical formulas are used to process data, these formulas can be found in papers from <code>References</code> section.</p>"},{"location":"System%20Design/#tools","title":"Tools","text":"<p>Under application we provide main software and tools such as Signal Simulator &amp; Script Monitor. These tools can be found on GitHub Repo releases.</p> <p>Currently, we have two tools</p> <ul> <li> <p>Signal Simulator - The actual signals are generated through MEA that are implanted into the subjects brain. But for testing the system software we simulate signals using this tools. It mimics the FreeEEG board.</p> </li> <li> <p>Scripts Monitor - The data is being published on different Mqtt topics by scripts and some scripts simultaneously. While development it becomes messy to open multiple terminal windows and look at each of them one by one. This tools lets you see what's being published on topics continously.</p> </li> </ul>"},{"location":"System%20Design/#dashboard","title":"Dashboard","text":"<p>The main software has a sci-fi user interface and delivers expectional performance. This is used to monitor everything including the subject's stress, reward, signals, data visualization, data logging etc. </p> UI under development <p>The interface is built using React JS and CSS. To make it run out-of-browser we use Electron JS. The application communicates with the backend written in Python using WebSockets and the scripts communicates through <code>Mqtt</code> Pub-Sub system.</p> <p>There are following files:</p> <ul> <li> <p>data_manager.py: It has <code>DataManager</code> class which provides functions to publish data and listen to incoming messages concurrently. For more details refer to Data Manager.</p> </li> <li> <p>constants.py: This file in the root directory contains constants and data strcutures with appropraite class methods for their conversion.</p> </li> <li> <p>signal_simulator.py: It generates synthetic signals for testing purposes.</p> </li> <li> <p>signals_to_features.py: It receives the generated signals through topic simulated signals, upon receiving signals it extract features from it and send them to features_to_game.py</p> </li> <li> <p>features_to_game.py: VizDoom has its own set of commands. The features that are translated into actions are in raw form, which might not be understood by the Doom Engine. It converts these raw actions into game commands that are known by VizDoom.</p> </li> </ul>"},{"location":"Tools/Signal%20Simulator/","title":"Signal Simulator","text":"<p>The actual signals are captured throug MEA implanted in the subject's brain. But for testing software, we use simulated signals. This Signal Simulator tool generates signals and publishes them over the same topic as that of actual data. These signals are then received by the data manager instances subscribed to the topic.</p> <p>Note</p> <p>Please refer to Topics section in System Design to see active topics.</p> <p></p> <p>The UI is simple but it has a lot of parameter to input by the user. To make this process more simpler you have to input the values once, then you <code>Save</code> &amp; <code>Load</code> your configurations.</p> <p>Warning</p> <p>The Signals Simulator has some bugs and also we have not implemented error handling. See progress below.</p>"},{"location":"Tools/Signal%20Simulator/#progress","title":"Progress","text":"<ul> <li> Basic user interface</li> <li> Error handling</li> <li> Publish Messages</li> <li> Save &amp; Get values</li> <li> Stable Release</li> </ul>"},{"location":"Tools/Signal%20Simulator/#parameters-discription","title":"Parameters &amp; Discription","text":"Key Description <code>min_volt</code> 1 microvolt <code>max_volt</code> 8 microvolts <code>variability_factor</code> Direct mapping of player movement to variability factor, normalized to 0-1 <code>variance</code> Mapping door state to variance feature <code>std_dev</code> Mapping enemy type to standard deviation feature <code>rms_value</code> Player health state affects the RMS value feature <code>num_peaks</code> Number of peaks determined by exploring states <code>peak_height</code> Peak height influenced by level state <code>fractal_dimension</code> Action states influence the fractal dimension <code>window_size</code> Window size feature influenced by wall states <code>target_rate</code> Target rate is determined by the presence of any enemy type <code>min_freq</code> Minimum frequency affected by player movement <code>max_freq</code> Maximum frequency influenced by player health state <code>blend_factor</code> Static blend factor as a static state <code>global_sync_level</code> Global sync level determined by action state <code>pairwise_sync_level</code> Pairwise sync level affected by door state <code>sync_factor</code> Sync factor as a static value for simplicity <code>influence_factor</code> Influence factor derived from enemy type <code>max_influence</code> Maximum influence as a static maximum for the presence of any enemy <code>centroid_factor</code> Centroid factor and edge density factor as placeholders for sensory data encoding <code>edge_density_factor</code> Centroid factor and edge density factor as placeholders for sensory data encoding <code>complexity_factor</code> Example value for complexity factor in FFT <code>evolution_rate</code> Evolution rate as a static value for dynamic environmental changes <code>low_freq</code> Low frequency ranges influenced by exploring states <code>high_freq</code> High frequency ranges influenced by level states <code>causality_strength</code> Causality strength as a static value for interaction effects <code>num_imfs</code> Number of intrinsic mode functions (IMFs) as a static value for interaction effects <p>A dialog will open up asking for some more parameters </p>"}]}